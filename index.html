
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index,follow">
    <meta name="keywords" content="Nick Yue; Zhongqi Yue; Nanyang Technological University; Singapore; Causal Inference; Disentangled Representation; NTU; MLLM; Multimodal Large Language Model; Large Language Model; Post-training; RL; Reinforcement Learning; Transfer Learning; Generalization; ICCV; ECCV; CVPR; Few-shot Learning; Domain Adaptation; Computer Vision; Machine Learning">
    <link rel="author" href="https://yue-zhongqi.github.io/">
    <title>Zhongqi (Nick) Yue's Homepage</title>
    <link rel="stylesheet" href="style.css" type="text/css" />
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <meta name="description" content="Zhongqi (Nick) Yue's research on multimodal LLMs, diffusion models, and causal generalization.">

</head>


<body>
    <div id="container">
        <div id="header">
            <div class="header-content">
                <div class="profile-section">
                    <div class="profile-info">
                        <h1><a>Zhongqi (Nick) <span class="surname">Yue</span></a></h1>
                        <h2>Dr.</h2>
                        <div class="position-info">
                            <p class="title">Presidential Postdoctoral Fellow</p>
                            <p class="affiliation">Nanyang Technological University</p>
                            <p class="research-area">Building generalizable intelligence through multimodal and reasoning-aligned foundation models.</p>
                        </div>
                        <div class="contact-info">
                            <div class="email-cv-row">
                                <div class="email">
                                    <object id="object" data="assets/envelope.svg" width="18" height="18" type="image/svg+xml"></object>
                                    yuez0002[AT]gmail.com
                                </div>
                                <div class="cv-button-container">
                                    <a href="assets/cv.pdf" target="_blank" class="cv-button">
                                        ðŸ“„ CV
                                    </a>
                                </div>
                            </div>
                            <div class="social-links">
                                <a href="https://scholar.google.com/citations?user=7Iyz9ZYAAAAJ&hl=en" target="_blank" title="Google Scholar">
                                    <img src="assets/google_scholar.png" height="26px" alt="Google Scholar">
                                </a>
                                <a href="https://github.com/yue-zhongqi" target="_blank" title="GitHub">
                                    <img src="assets/github.png" height="26px" alt="GitHub">
                                </a>
                                <a href="https://sg.linkedin.com/in/yue-zhongqi" target="_blank" title="LinkedIn">
                                    <img src="assets/linkedin.png" height="26px" alt="LinkedIn">
                                </a>
                                <a href="https://dblp.org/pid/275/3790.html" target="_blank" title="DBLP">
                                    <img src="assets/dblp.png" height="26px" alt="DBLP">
                                </a>
                                <a href="https://mreallab.github.io/" target="_blank" title="Lab">
                                    <img src="assets/location.png" height="26px" alt="Lab">
                                </a>
                            </div>
                        </div>
                    </div>
                    <div class="profile-photo">
                        <img src="images/profile.png" alt="Zhongqi Yue">
                    </div>
                </div>
            </div>
            <div class="clear"></div>
        </div>


        
        <div id="body">
            <div id="Biography">
                <h2>Biography</h2>
                <div class="biography-content">
                    <p>I am a <a href="https://www.ntu.edu.sg/research/research-careers/presidential-postdoctoral-fellowship-(ppf)">Presidential Postdoctoral Fellow</a> (PPF, Principal Investigator), jointly affiliated with Nanyang Technological University and Chalmers University of Technology (with <a href="https://www.fredjo.com/">Prof. Fredrik D. Johansson</a>). In 2023, I completed my Ph.D. in NTU under <a href="https://www.ntu.edu.sg/alibaba-ntu-jri/programmes">Alibaba Talent Program</a>, supervised by <a href="https://scholar.google.com/citations?user=YG0DFyYAAAAJ&hl=en">Prof. Hanwang Zhang</a> and co-supervised by <a href="https://qianrusun.com/">Prof. Qianru Sun</a>. During Ph.D., I did an internship in Sea working under <a href="https://panzhous.github.io/">Prof. Pan Zhou</a>. Prior to that, I received my bachelor's degree from NTU in 2017 under MOE SM2 scholarship.</p>
                    
                    <p>My research lies at the intersection of multimodal LLMs (MLLMs), representation learning, and causal generalization, with the broader goal of building AI systems that can learn and reason beyond language. I have published in top-tier venues including NeurIPS, ICLR, CVPR, and ICCV, receiving multiple oral and spotlight recognitions and a CVPR 2025 Best Student Paper Honorable Mention. My recent works explore post-training for LLMs and MLLMs to advance reasoning, grounding, and agentic capabilities in foundation models.</p>
                </div>
            </div>


            <div id="News">
                <h2>News</h2>
                <div class="news-container">
                    <div class="news-item">
                        <span class="news-date">[10, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">NeurIPS 2025</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">ICCV 2025</span> (1 highlight).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">CVPR 2025</span> (1 best student paper honorable mention and 1 oral presentation).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2025]</span>
                        <span class="news-content">Released Selftok technical report (image tokenization, MLLM pre-training and post-training).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[02, 2025]</span>
                        <span class="news-content">Continued the PPF in Chalmers University of Technology, Sweden.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2024]</span>
                        <span class="news-content">1 paper about few-shot learning accepted by <span class="venue">CVPR 2024</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2024]</span>
                        <span class="news-content">1 paper about unsupervised representation learning accepted by <span class="venue">ICLR 2024</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[02, 2024]</span>
                        <span class="news-content">Started the PPF in NTU.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[10, 2023]</span>
                        <span class="news-content">Started a research internship in Sea.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2023]</span>
                        <span class="news-content">1 paper about unsupervised domain adaptation accepted by <span class="venue">NeurIPS 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[08, 2023]</span>
                        <span class="news-content">Awarded <span class="award">Wallenberg-NTU Presidential Postdoctoral Fellowship</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[07, 2023]</span>
                        <span class="news-content">2 papers about open-world detection and fair face recognition are accepted by <span class="venue">ICCV 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[03, 2023]</span>
                        <span class="news-content">1 paper about video anomaly detection accepted by <span class="venue">CVPR 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[08, 2022]</span>
                        <span class="news-content">Received 2022 <a href="http://www.premiasg.org/">PREMIA</a> Best Student Paper Awards (The Gold Award).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2021]</span>
                        <span class="news-content">1 paper about self-supervised learning accepted by <span class="venue">NeurIPS 2022</span> (Spotlight).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[07, 2021]</span>
                        <span class="news-content">1 paper about unsupervised domain adaptation accepted by <span class="venue">ICCV 2021</span> (Oral).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[03, 2021]</span>
                        <span class="news-content">1 paper about zero-shot learning accepted by <span class="venue">CVPR 2021</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2020]</span>
                        <span class="news-content">1 paper about few-shot learning accepted by <span class="venue">NeurIPS 2020</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2020]</span>
                        <span class="news-content">Joined <span class="program">Alibaba Talent Program</span> to do a Ph.D. in NTU.</span>
                    </div>
                </div>      
            </div>
            
            <div id="Publications">
                <h2>Publications<span class="section-link"> [<a href="https://scholar.google.com/citations?user=7Iyz9ZYAAAAJ&hl=en" target="_blank">Google Scholar</a>]</span></h2>
                
                <div class="publication-category">
                    <h3 class="category-title" onclick="toggleCategory(this)">LLMs and MLLMs</h3>
                    <div class="publications-grid">
                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://expa-rl.github.io/" target="_blank">
                                    <img src="images/expa.png" alt="EXPA-RL paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Expanding the Action Space of LLMs to Reason Beyond Language</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Weishi Wang*, Yundaichuan Zhan, Juncheng Li, Daniel Dahlmeier, Fredrik D. Johansson</p>
                                <p class="pub-venue">2025</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/pdf/2510.07581" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://expa-rl.github.io/" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://ddt-llama.github.io/" target="_blank">
                                    <img src="images/ddt-llama.png" alt="DDT-LLaMA paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens</h3>
                                <p class="pub-authors">Kaihang Pan*, Wang Lin*, <strong>Zhongqi Yue*</strong>, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, Hanwang Zhang</p>
                                <p class="pub-venue">CVPR 2025</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Best Student Paper Honorable Mention 7/13008</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2504.14666" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://ddt-llama.github.io/" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://selftok-team.github.io/report/" target="_blank">
                                    <img src="images/selftok.png" alt="Selftok paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning</h3>
                                <p class="pub-authors">Bohan Wang, <strong>Zhongqi Yue</strong>, Fengda Zhang, Shuo Chen, Li'an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang</p>
                                <p class="pub-venue">Technical Report</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2505.07538" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://selftok-team.github.io/report/" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="#" target="_blank">
                                    <img src="images/selftok-zero.png" alt="Selftok-Zero paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Selftok-Zero: Reinforcement Learning for Visual Generation via Discrete and Autoregressive Visual Tokens</h3>
                                <p class="pub-authors">Bohan Wang, Mingze Zhou, <strong>Zhongqi Yue</strong>, Wang Lin, Kaihang Pan, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang</p>
                                <p class="pub-venue">NeurIPS 2025</p>
                                <div class="pub-links">
                                    <a href="#" target="_blank" class="paper-link">Paper Coming Soon</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/Yuqifan1117/DataTailor" target="_blank">
                                    <img src="images/mastering_collab.png" alt="DataTailor paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Mastering Collaborative Multi-Modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness</h3>
                                <p class="pub-authors">Qifan Yu*, Zhebei Shen*, <strong>Zhongqi Yue*</strong>, Yang Wu, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</p>
                                <p class="pub-venue">ICCV 2025</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Highlight 262/11239</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2412.06293" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/Yuqifan1117/DataTailor" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://dcd-anyedit.github.io/" target="_blank">
                                    <img src="images/anyedit.png" alt="AnyEdit paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</h3>
                                <p class="pub-authors">Qifan Yu*, Wei Chow*, <strong>Zhongqi Yue*</strong>, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang</p>
                                <p class="pub-venue">CVPR 2025</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Oral 96/13008</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2411.15738" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://dcd-anyedit.github.io/" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="#" target="_blank">
                                    <img src="images/counterfactual_evo.png" alt="Counterfactual Evolution paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Counterfactual Evolution of Multimodal Datasets via Visual Programming</h3>
                                <p class="pub-authors">Minghe Gao*, <strong>Zhongqi Yue*</strong>, Wenjie Yan, Yihao Hu, Wei Ji, Siliang Tang, Jun Xiao, Tat-Seng Chua, Yueting Zhuang, Juncheng Li</p>
                                <p class="pub-venue">NeurIPS 2025</p>
                                <div class="pub-links">
                                    <a href="#" target="_blank" class="paper-link">Paper Coming Soon</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/minghehe-nobug/SVIP" target="_blank">
                                    <img src="images/multimodal_cot.png" alt="SVIP paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program</h3>
                                <p class="pub-authors">Minghe Gao, Xuqi Liu, <strong>Zhongqi Yue</strong>, Yang Wu, Shuang Chen, Juncheng Li, Siliang Tang, Fei Wu, Tat-Seng Chua, Yueting Zhuang</p>
                                <p class="pub-venue">ICCV 2025</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2504.06606" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/minghehe-nobug/SVIP" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="publication-category">
                    <h3 class="category-title" onclick="toggleCategory(this)">Representation Learning</h3>
                    <div class="publications-grid">
                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/Wangt-CN/IP-IRM" target="_blank">
                                    <img src="images/ipirm.PNG" alt="IP-IRM paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Self-Supervised Learning Disentangled Group Representation as Feature</h3>
                                <p class="pub-authors">Tan Wang, <strong>Zhongqi Yue</strong>, Jianqiang Huang, Qianru Sun, Hanwang Zhang</p>
                                <p class="pub-venue">NeurIPS 2021</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Spotlight Presentation 260/9122</span>
                                    <span class="highlight">PREMIA Best Student Paper 2022</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2110.15255" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/Wangt-CN/IP-IRM" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/diti" target="_blank">
                                    <img src="images/diti.png" alt="DiTi paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Exploring Diffusion Time-Steps for Unsupervised Representation Learning</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang</p>
                                <p class="pub-venue">ICLR 2024</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2401.11430" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/diti" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/milliema/InvReg" target="_blank">
                                    <img src="images/ny_invreg.PNG" alt="InvReg paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Invariant Feature Regularization for Fair Face Recognition</h3>
                                <p class="pub-authors">Jiali Ma, <strong>Zhongqi Yue</strong>, Tomoyuki Kagaya, Tomoki Suzuki, Karlekar Jayashree, Sugiri Pranata, Hanwang Zhang</p>
                                <p class="pub-venue">ICCV 2023</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2310.14652" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/milliema/InvReg" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="publication-category">
                    <h3 class="category-title" onclick="toggleCategory(this)">Generalization</h3>
                    <div class="publications-grid">
                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/tif" target="_blank">
                                    <img src="images/tif.png" alt="TiF paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Few-Shot Learner Parameterization by Diffusion Time-Steps</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun</p>
                                <p class="pub-venue">CVPR 2024</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2403.02649" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/tif" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/tcm" target="_blank">
                                    <img src="images/tcm.PNG" alt="TCM paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Transporting Causal Mechanisms for Unsupervised Domain Adaptation</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">ICCV 2021</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Oral Presentation 210/6236</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2107.11055" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/tcm" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/icon" target="_blank">
                                    <img src="images/ny_icon.PNG" alt="ICON paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Hanwang Zhang, Qianru Sun</p>
                                <p class="pub-venue">NeurIPS 2023</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2309.12742" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/icon" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/scuwyh2000/RandBox" target="_blank">
                                    <img src="images/randbox.PNG" alt="RandBox paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Random Boxes Are Open-world Object Detectors</h3>
                                <p class="pub-authors">Yanghao Wang, <strong>Zhongqi Yue</strong>, Xian-Sheng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">ICCV 2023</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2307.08249" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/scuwyh2000/RandBox" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/ktr-hubrt/UMIL" target="_blank">
                                    <img src="images/umil.PNG" alt="UMIL paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection</h3>
                                <p class="pub-authors">Hui Lv, <strong>Zhongqi Yue</strong>, Qianru Sun, Bin Luo, Zhen Cui, Hanwang Zhang</p>
                                <p class="pub-venue">CVPR 2023</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2303.12369" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/ktr-hubrt/UMIL" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/gcm-cf" target="_blank">
                                    <img src="images/gcmcf.PNG" alt="GCM-CF paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Counterfactual Zero-Shot and Open-Set Visual Recognition</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue*</strong>, Tan Wang*, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">CVPR 2021</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2103.00887" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/gcm-cf" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://github.com/yue-zhongqi/ifsl" target="_blank">
                                    <img src="images/ifsl.PNG" alt="IFSL paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Interventional Few-Shot Learning</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua</p>
                                <p class="pub-venue">NeurIPS 2020</p>
                                <div class="pub-links">
                                    <a href="https://arxiv.org/abs/2009.13000" target="_blank" class="paper-link">Paper</a>
                                    <a href="https://github.com/yue-zhongqi/ifsl" target="_blank" class="project-link">Project Page</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <footer class="page-footer">
                <div class="footer-content">
                    <p>&copy; 2025 Zhongqi Yue | Last updated: October 2025</p>
                </div>
            </footer>

        <div class="clear"></div>
    </div>

    <script>
        function toggleCategory(titleElement) {
            const publicationsGrid = titleElement.nextElementSibling;
            const isCollapsed = publicationsGrid.classList.contains('collapsed');
            
            if (isCollapsed) {
                // Expanding: first remove collapsed class, measure height, then animate
                publicationsGrid.classList.remove('collapsed');
                const height = publicationsGrid.scrollHeight;
                publicationsGrid.style.maxHeight = '0px';
                publicationsGrid.style.opacity = '0';
                
                // Force a reflow
                publicationsGrid.offsetHeight;
                
                // Now animate to the measured height
                publicationsGrid.style.maxHeight = height + 'px';
                publicationsGrid.style.opacity = '1';
                
                titleElement.classList.remove('collapsed');
                
                // After animation completes, remove the fixed height so it can adapt to content changes
                setTimeout(() => {
                    if (!publicationsGrid.classList.contains('collapsed')) {
                        publicationsGrid.style.maxHeight = 'none';
                    }
                }, 300);
                
            } else {
                // Collapsing: set current height, then animate to 0
                const height = publicationsGrid.scrollHeight;
                publicationsGrid.style.maxHeight = height + 'px';
                
                // Force a reflow
                publicationsGrid.offsetHeight;
                
                // Now animate to collapsed state
                publicationsGrid.style.maxHeight = '0px';
                publicationsGrid.style.opacity = '0';
                
                titleElement.classList.add('collapsed');
                
                // After animation completes, add the collapsed class
                setTimeout(() => {
                    publicationsGrid.classList.add('collapsed');
                }, 300);
            }
        }
        
        // Handle window resize to ensure expanded sections maintain proper height
        window.addEventListener('resize', function() {
            document.querySelectorAll('.publications-grid:not(.collapsed)').forEach(function(grid) {
                if (grid.style.maxHeight && grid.style.maxHeight !== 'none') {
                    grid.style.maxHeight = 'none';
                }
            });
        });
    </script>
</body>

</html>

