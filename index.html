
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index,follow">
    <meta name="keywords" content="Nick Yue; Zhongqi Yue; Nanyang Technological University; Singapore; Causal Inference; Disentangled Representation; NTU; MLLM; Multimodal Large Language Model; Large Language Model; Post-training; RL; Reinforcement Learning; Transfer Learning; Generalization; ICCV; ECCV; CVPR; Few-shot Learning; Domain Adaptation; Computer Vision; Machine Learning">
    <link rel="author" href="https://yue-zhongqi.github.io/">
    <title>Zhongqi (Nick) Yue's Homepage</title>
    <link rel="stylesheet" href="style.css" type="text/css" />
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <meta name="description" content="Zhongqi (Nick) Yue's research on multimodal LLMs, diffusion models, and causal generalization.">

</head>


<body>
    <div id="container">
        <div id="header">
            <div class="header-content">
                <div class="profile-section">
                    <div class="profile-info">
                        <h1><a>Zhongqi <span class="surname">Yue</span></a></h1>
                        <h2>Dr.</h2>
                        <div class="position-info">
                            <p class="title">Presidential Postdoctoral Fellow</p>
                            <p class="affiliation">Nanyang Technological University</p>
                            <p class="research-area">Building generalizable intelligence through multimodal and reasoning-aligned foundation models.</p>
                        </div>
                        <div class="contact-info">
                            <div class="email">
                                <object id="object" data="assets/envelope.svg" width="18" height="18" type="image/svg+xml"></object>
                                yuez0002[AT]gmail.com
                            </div>
                            <div class="social-links">
                                <a href="https://scholar.google.com/citations?user=7Iyz9ZYAAAAJ&hl=en" target="_blank" title="Google Scholar">
                                    <img src="assets/google_scholar.png" height="26px" alt="Google Scholar">
                                </a>
                                <a href="https://github.com/yue-zhongqi" target="_blank" title="GitHub">
                                    <img src="assets/github.png" height="26px" alt="GitHub">
                                </a>
                                <a href="https://sg.linkedin.com/in/yue-zhongqi" target="_blank" title="LinkedIn">
                                    <img src="assets/linkedin.png" height="26px" alt="LinkedIn">
                                </a>
                                <a href="https://dblp.org/pid/275/3790.html" target="_blank" title="DBLP">
                                    <img src="assets/dblp.png" height="26px" alt="DBLP">
                                </a>
                                <a href="https://mreallab.github.io/" target="_blank" title="Lab">
                                    <img src="assets/location.png" height="26px" alt="Lab">
                                </a>
                            </div>
                        </div>
                    </div>
                    <div class="profile-photo">
                        <img src="images/profile.png" alt="Zhongqi Yue">
                    </div>
                </div>
            </div>
            <div class="clear"></div>
        </div>


        
        <div id="body">
            <div id="Biography">
                <h2>Biography</h2>
                <div class="biography-content">
                    <p>I am a <a href="https://www.ntu.edu.sg/research/research-careers/presidential-postdoctoral-fellowship-(ppf)">Presidential Postdoctoral Fellow</a> (PPF, Principal Investigator) at Nanyang Technological University (<a href="https://www.ntu.edu.sg/">NTU</a>) in Singapore. In 2023, I completed my Ph.D. in NTU under <a href="https://www.ntu.edu.sg/alibaba-ntu-jri/programmes">Alibaba Talent Program</a>, supervised by <a href="https://scholar.google.com/citations?user=YG0DFyYAAAAJ&hl=en">Prof. Hanwang Zhang</a>. During Ph.D., I did an internship in Sea working under <a href="https://panzhous.github.io/">Dr. Pan Zhou</a>. Prior to that, I received my bachelor's degree from NTU in 2017 under MOE SM2 scholarship.</p>
                    
                    <p>My research lies at the intersection of multimodal LLMs (MLLMs), representation learning, and causal generalization, with the broader goal of building AI systems that can learn and reason beyond language. I have published in top-tier venues including NeurIPS, ICLR, CVPR, and ICCV, receiving multiple oral and spotlight recognitions and a CVPR 2025 Best Student Paper Honorable Mention. My recent works explore post-training for LLMs and MLLMs to advance reasoning, grounding, and agentic capabilities in foundation models.</p>
                </div>
            </div>


            <div id="News">
                <h2>News</h2>
                <div class="news-container">
                    <div class="news-item">
                        <span class="news-date">[10, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">NeurIPS 2025</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">ICCV 2025</span> (1 highlight).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2025]</span>
                        <span class="news-content">2 papers accepted by <span class="venue">CVPR 2025</span> (1 best student paper honorable mention and 1 oral presentation).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2025]</span>
                        <span class="news-content">Released Selftok technical report (image tokenization, MLLM pre-training and post-training).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[02, 2025]</span>
                        <span class="news-content">Continued the PPF in Chalmers University of Technology, Sweden.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[06, 2024]</span>
                        <span class="news-content">1 paper about few-shot learning accepted by <span class="venue">CVPR 2024</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2024]</span>
                        <span class="news-content">1 paper about unsupervised representation learning accepted by <span class="venue">ICLR 2024</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[02, 2024]</span>
                        <span class="news-content">Started the PPF in NTU.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[10, 2023]</span>
                        <span class="news-content">Started a research internship in Sea.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2023]</span>
                        <span class="news-content">1 paper about unsupervised domain adaptation accepted by <span class="venue">NeurIPS 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[08, 2023]</span>
                        <span class="news-content">Awarded <span class="award">Wallenberg-NTU Presidential Postdoctoral Fellowship</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[07, 2023]</span>
                        <span class="news-content">2 papers about open-world detection and fair face recognition are accepted by <span class="venue">ICCV 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[03, 2023]</span>
                        <span class="news-content">1 paper about video anomaly detection accepted by <span class="venue">CVPR 2023</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[08, 2022]</span>
                        <span class="news-content">Received 2022 <a href="http://www.premiasg.org/">PREMIA</a> Best Student Paper Awards (The Gold Award).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2021]</span>
                        <span class="news-content">1 paper about self-supervised learning accepted by <span class="venue">NeurIPS 2022</span> (Spotlight).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[07, 2021]</span>
                        <span class="news-content">1 paper about unsupervised domain adaptation accepted by <span class="venue">ICCV 2021</span> (Oral).</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[03, 2021]</span>
                        <span class="news-content">1 paper about zero-shot learning accepted by <span class="venue">CVPR 2021</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[09, 2020]</span>
                        <span class="news-content">1 paper about few-shot learning accepted by <span class="venue">NeurIPS 2020</span>.</span>
                    </div>
                    <div class="news-item">
                        <span class="news-date">[05, 2020]</span>
                        <span class="news-content">Joined <span class="program">Alibaba Talent Program</span> to do a Ph.D. in NTU.</span>
                    </div>
                </div>      
            </div>
            
            <div id="Publications">
                <h2>Publications<span class="section-link"> [<a href="https://scholar.google.com/citations?user=7Iyz9ZYAAAAJ&hl=en" target="_blank">Google Scholar</a>]</span></h2>
                
                <div class="publication-category">
                    <h3 class="category-title" onclick="toggleCategory(this)">Representation Learning</h3>
                    <div class="publications-grid">
                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2309.12742" target="_blank">
                                    <img src="images/ny_icon.PNG" alt="ICON paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Hanwang Zhang, Qianru Sun</p>
                                <p class="pub-venue">Conference on Neural Information Processing Systems (NeurIPS), 2023</p>
                                <div class="pub-highlights">
                                    <span class="highlight">WILDS 2.0 Leaderboard 1st Place (with unlabeled data)</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://github.com/yue-zhongqi/icon" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Invariant_Feature_Regularization_for_Fair_Face_Recognition_ICCV_2023_paper.pdf" target="_blank">
                                    <img src="images/ny_invreg.PNG" alt="InvReg paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Invariant Feature Regularization for Fair Face Recognition</h3>
                                <p class="pub-authors">Jiali Ma, <strong>Zhongqi Yue</strong>, Tomoyuki Kagaya, Tomoki Suzuki, Karlekar Jayashree, Sugiri Pranata, Hanwang Zhang</p>
                                <p class="pub-venue">International Conference on Computer Vision (ICCV), 2023</p>
                                <div class="pub-links">
                                    <a href="https://github.com/milliema/InvReg" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2307.08249" target="_blank">
                                    <img src="images/randbox.PNG" alt="RandBox paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Random Boxes Are Open窶層orld Object Detectors</h3>
                                <p class="pub-authors">Yanghao Wang, <strong>Zhongqi Yue</strong>, Xian窶全heng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">International Conference on Computer Vision (ICCV), 2023</p>
                                <div class="pub-links">
                                    <a href="https://github.com/scuwyh2000/RandBox" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2303.12369" target="_blank">
                                    <img src="images/umil.PNG" alt="UMIL paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection</h3>
                                <p class="pub-authors">Hui Lv, <strong>Zhongqi Yue</strong>, Qianru Sun, Bin Luo, Zhen Cui, Hanwang Zhang</p>
                                <p class="pub-venue">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
                                <div class="pub-links">
                                    <a href="https://github.com/ktr-hubrt/UMIL" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2110.15255" target="_blank">
                                    <img src="images/ipirm.PNG" alt="IP-IRM paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Self-Supervised Learning Disentangled Group Representation as Feature</h3>
                                <p class="pub-authors">Tan Wang, <strong>Zhongqi Yue</strong>, Jianqiang Huang, Qianru Sun, Hanwang Zhang</p>
                                <p class="pub-venue">Conference on Neural Information Processing Systems (NeurIPS), 2021</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Spotlight Presentation 260/9122</span>
                                    <span class="highlight">2022 PREMIA Best Student Paper</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://github.com/Wangt-CN/IP-IRM" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2110.15255" target="_blank">
                                    <img src="images/tcm.PNG" alt="TCM paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Transporting Causal Mechanisms for Unsupervised Domain Adaptation</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Qianru Sun, Xian窶全heng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">International Conference on Computer Vision (ICCV), 2021</p>
                                <div class="pub-highlights">
                                    <span class="highlight">Oral Presentation 210/6236</span>
                                </div>
                                <div class="pub-links">
                                    <a href="https://github.com/yue-zhongqi/tcm" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="publication-category">
                    <h3 class="category-title" onclick="toggleCategory(this)">Transfer Learning</h3>
                    <div class="publications-grid">
                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yue_Counterfactual_Zero-Shot_and_Open-Set_Visual_Recognition_CVPR_2021_paper.pdf" target="_blank">
                                    <img src="images/gcmcf.PNG" alt="GCM-CF paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Counterfactual Zero窶全hot and Open窶全et Visual Recognition</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue*</strong>, Tan Wang*, Qianru Sun, Xian窶全heng Hua, Hanwang Zhang</p>
                                <p class="pub-venue">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</p>
                                <div class="pub-links">
                                    <a href="https://github.com/yue-zhongqi/gcm-cf" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="pub-image">
                                <a href="https://arxiv.org/abs/2009.13000" target="_blank">
                                    <img src="images/ifsl.PNG" alt="IFSL paper">
                                </a>
                            </div>
                            <div class="pub-content">
                                <h3 class="pub-title">Interventional Few窶全hot Learning</h3>
                                <p class="pub-authors"><strong>Zhongqi Yue</strong>, Hanwang Zhang, Qianru Sun, Xian窶全heng Hua</p>
                                <p class="pub-venue">Conference on Neural Information Processing Systems (NeurIPS), 2020</p>
                                <div class="pub-links">
                                    <a href="https://github.com/yue-zhongqi/ifsl" target="_blank" class="project-link">沐･ Project Page 沐･</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <footer class="page-footer">
                <div class="footer-content">
                    <p>&copy; 2024 Zhongqi Yue | Last updated: October 2024</p>
                </div>
            </footer>

        <div class="clear"></div>
    </div>

    <script>
        function toggleCategory(titleElement) {
            const publicationsGrid = titleElement.nextElementSibling;
            const isCollapsed = publicationsGrid.classList.contains('collapsed');
            
            if (isCollapsed) {
                // Expanding: first remove collapsed class, measure height, then animate
                publicationsGrid.classList.remove('collapsed');
                const height = publicationsGrid.scrollHeight;
                publicationsGrid.style.maxHeight = '0px';
                publicationsGrid.style.opacity = '0';
                
                // Force a reflow
                publicationsGrid.offsetHeight;
                
                // Now animate to the measured height
                publicationsGrid.style.maxHeight = height + 'px';
                publicationsGrid.style.opacity = '1';
                
                titleElement.classList.remove('collapsed');
                
                // After animation completes, remove the fixed height so it can adapt to content changes
                setTimeout(() => {
                    if (!publicationsGrid.classList.contains('collapsed')) {
                        publicationsGrid.style.maxHeight = 'none';
                    }
                }, 300);
                
            } else {
                // Collapsing: set current height, then animate to 0
                const height = publicationsGrid.scrollHeight;
                publicationsGrid.style.maxHeight = height + 'px';
                
                // Force a reflow
                publicationsGrid.offsetHeight;
                
                // Now animate to collapsed state
                publicationsGrid.style.maxHeight = '0px';
                publicationsGrid.style.opacity = '0';
                
                titleElement.classList.add('collapsed');
                
                // After animation completes, add the collapsed class
                setTimeout(() => {
                    publicationsGrid.classList.add('collapsed');
                }, 300);
            }
        }
        
        // Handle window resize to ensure expanded sections maintain proper height
        window.addEventListener('resize', function() {
            document.querySelectorAll('.publications-grid:not(.collapsed)').forEach(function(grid) {
                if (grid.style.maxHeight && grid.style.maxHeight !== 'none') {
                    grid.style.maxHeight = 'none';
                }
            });
        });
    </script>
</body>

</html>

